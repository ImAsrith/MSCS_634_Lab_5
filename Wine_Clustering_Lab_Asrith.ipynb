{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b195c406",
   "metadata": {},
   "source": [
    "# Vejandla Asrith\n",
    "\n",
    "**Course:** MSCS 634 – Data Mining  \n",
    "\n",
    "**Lab Assignment:** Wine Clustering Walk‑Through  \n",
    "\n",
    "---  \n",
    "\n",
    "In this notebook I roll up my sleeves and get comfortable with two clustering heavy‑hitters—Hierarchical (Agglomerative) and DBSCAN—using the classic Wine dataset. The goal is to see how the algorithms behave, how parameter tweaks shift the story, and what the evaluation metrics whisper back about cluster quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea5ba5",
   "metadata": {},
   "source": [
    "## 1. Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the Wine dataset straight from sklearn—no hunting for CSVs today.\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "wine_bunch = load_wine()\n",
    "X_raw = pd.DataFrame(wine_bunch.data, columns=wine_bunch.feature_names)\n",
    "y_target = wine_bunch.target  # We will not cluster on this, but it helps later for scores\n",
    "\n",
    "# A quick peek at the shape and first rows\n",
    "display(X_raw.head())\n",
    "X_raw.info()\n",
    "display(X_raw.describe())\n",
    "\n",
    "# Standardizing so every feature plays on the same field\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf83d2",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a501c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Reduce dimensionality for clean visuals\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca.fit_transform(X_scaled)\n",
    "\n",
    "cluster_options = [2, 3, 4, 5]\n",
    "for k in cluster_options:\n",
    "    model = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = model.fit_predict(X_scaled)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels)\n",
    "    plt.title(f'Agglomerative Clustering (k = {k})')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.show()\n",
    "\n",
    "# Dendrogram for a bird’s‑eye view of linkage distances\n",
    "link_matrix = linkage(X_scaled, method='ward')\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(link_matrix, truncate_mode='level', p=5)\n",
    "plt.title('Wine Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee5212",
   "metadata": {},
   "source": [
    "## 3. DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, homogeneity_score, completeness_score\n",
    "import numpy as np\n",
    "\n",
    "db_params = [(0.5, 5), (0.7, 5), (0.5, 10)]\n",
    "results = []\n",
    "\n",
    "for eps, min_samples in db_params:\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    db_labels = db.fit_predict(X_scaled)\n",
    "\n",
    "    # Visualize clusters (+ noise in label -1)\n",
    "    plt.figure()\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=db_labels)\n",
    "    plt.title(f'DBSCAN (eps={eps}, min_samples={min_samples})')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics—mind the single‑cluster or all‑noise edge cases\n",
    "    unique = set(db_labels)\n",
    "    cluster_count = len([lab for lab in unique if lab != -1])\n",
    "    sil = silhouette_score(X_scaled, db_labels) if cluster_count > 1 else np.nan\n",
    "    homo = homogeneity_score(y_target, db_labels)\n",
    "    comp = completeness_score(y_target, db_labels)\n",
    "    results.append((eps, min_samples, cluster_count, sil, homo, comp))\n",
    "\n",
    "# Display metric table\n",
    "results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'clusters', 'silhouette', 'homogeneity', 'completeness'])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce9f05",
   "metadata": {},
   "source": [
    "## 4. Analysis & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c4427",
   "metadata": {},
   "source": [
    "After sampling different settings, the hierarchical approach naturally segments the wines into intuitive groups when k is set to 3, echoing the original dataset’s three wine classes. The dendrogram visually reinforces that three‑cluster cut—there’s a pronounced jump in linkage distance beyond that point.\n",
    "\n",
    "DBSCAN behaves as expected: with a modest `eps` of 0.5 it struggles to gather points into more than one dense region, flagging much of the space as noise. Easing the neighborhood radius to 0.7 gives us two clusters plus outliers, but silhouette drops, reflecting overlapping regions in the feature space. Raising `min_samples` to 10 tightens density requirements, again yielding sparse grouping and more noise.\n",
    "\n",
    "**Strengths observed**  \n",
    "*Hierarchical* provides that useful dendrogram narrative, revealing potential cluster counts without guesswork. Its deterministic nature also ensures reproducibility.  \n",
    "*DBSCAN* excels at identifying arbitrarily‑shaped clusters and calling out noise, something k‑based methods ignore. That said, its sensitivity to `eps` can be finicky on standardized but still high‑dimensional data.\n",
    "\n",
    "**Weaknesses noted**  \n",
    "Hierarchical’s Ward linkage leans toward spherical clusters and isn’t fond of large datasets. DBSCAN, while robust to outliers, can collapse to “everything is noise” if `eps` is a hair off, making parameter tuning part science, part art.\n",
    "\n",
    "In this wine experiment, hierarchical wins on interpretability and stability, whereas DBSCAN’s magic is muted because the data naturally separates into fairly compact groups rather than intricate shapes."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
